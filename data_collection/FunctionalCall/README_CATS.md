# Function Call Evaluation with CATS

This directory contains scripts for evaluating function calling capabilities of language models using the CATS framework with the STOP (Spoken Task Oriented Semantic Parsing) dataset.

## Overview

The function call evaluation has been integrated with the CATS framework to allow consistent evaluation of audio language models across multiple tasks. The integration includes:

1. Support for function calling in the main CATS inference pipeline
2. Tools to convert the STOP dataset to CATS format
3. A specialized inference script for function calling tasks

## Getting Started

### 1. Download the STOP dataset

```bash
wget https://dl.fbaipublicfiles.com/stop/stop.tar.gz
tar -xzf stop.tar.gz
```

### 2. Convert the STOP dataset to CATS format

Use the `convert_stop_to_cats.py` script to convert the STOP dataset to the format required by CATS:

```bash
python data_collection/FunctionalCall/convert_stop_to_cats.py \
    --stop_dataset path/to/stop \
    --audio_dir path/to/stop/audio \
    --output_dir data
```

This will:
- Extract function definitions from the STOP dataset and save them as `data/openai_functions.json`
- Extract utterances and their corresponding function calls and save them as `data/function_call_inputs.jsonl`
- Copy audio files to `data/function_calling/audio/`

### 3. Run the evaluation

#### Option 1: Using the specialized inference script

```bash
python data_collection/FunctionalCall/cats_inference_functions.py \
    --model gpt-4o-audio \
    --functions_file data/openai_functions.json \
    --data_file data/function_call_inputs.jsonl \
    --audio_dir function_calling/audio/
```

#### Option 2: Using the main CATS inference pipeline

The main CATS inference pipeline now supports function calling:

```python
from cats.inference import main as cats_main
from cats.inference import reset_api_counters, load_model, run_evaluation
from cats.config import create_task_configs
import json

# Load functions from file
with open("data/openai_functions.json", "r") as f:
    functions = json.load(f)

# Reset API counters
reset_api_counters()

# Get the function_call task
tasks = create_task_configs()
task_config = tasks["function_call"]

# Update the task config with functions
task_config = task_config._replace(
    functions=functions,
    use_functions=True
)

# Load the model
model_resources = load_model("gpt-4o-audio")

# Run evaluation
accuracy, results = run_evaluation(model_resources, task_config)
```

## Understanding the Results

The evaluation generates output files in the format:
`function_call_inputs.jsonl_[MODEL_NAME]_function_call`

Each record in the output file contains the original data plus:
- `predicted_function_call`: The function call generated by the model
- Error information (if any)

The accuracy is calculated by comparing the function names in the ground truth and predicted function calls. A more detailed evaluation can be performed by examining:
- Function name match
- Parameter matches
- Nested function call handling

## Additional Scripts

### `eval_function_call.py`

The original evaluation script for the STOP dataset. It provides more detailed function call comparison, including:
- Full parameter checking
- Nested function call support
- Order checking

### `extract_openai_functions.py`

Extracts OpenAI-compatible function definitions from the STOP dataset.

### `extract_stop_utterances.py`

Extracts utterances from the STOP dataset.

## Extending to Other Datasets

To use function calling evaluation with other datasets:

1. Format your data as a JSONL file with entries containing:
   ```json
   {
     "filename": "audio_file.wav",
     "transcript": "optional transcript",
     "function_call": {
       "name": "function_name",
       "arguments": {"param1": "value1", "param2": "value2"}
     }
   }
   ```

2. Create a functions definition file in OpenAI format:
   ```json
   [
     {
       "name": "function_name",
       "description": "Description of the function",
       "parameters": {
         "type": "object",
         "properties": {
           "param1": {"type": "string", "description": "Parameter description"},
           "param2": {"type": "string", "description": "Parameter description"}
         },
         "required": ["param1"]
       }
     }
   ]
   ```

3. Run the evaluation using the specialized inference script or the main CATS pipeline.